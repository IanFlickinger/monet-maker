{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# math and ml\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport numpy as np\n\n# visualization\nimport matplotlib.pyplot as plt\nimport PIL\nimport IPython\n\n# environment configuration\nimport kaggle_datasets\nimport os\n\n# submission handling\nimport shutil","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-03T17:58:21.17678Z","iopub.execute_input":"2022-11-03T17:58:21.177092Z","iopub.status.idle":"2022-11-03T17:58:28.285731Z","shell.execute_reply.started":"2022-11-03T17:58:21.177019Z","shell.execute_reply":"2022-11-03T17:58:28.284449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mute tensorflow logging\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = str(3)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-11-03T17:58:28.2892Z","iopub.execute_input":"2022-11-03T17:58:28.289821Z","iopub.status.idle":"2022-11-03T17:58:28.300507Z","shell.execute_reply.started":"2022-11-03T17:58:28.289775Z","shell.execute_reply":"2022-11-03T17:58:28.298574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configs","metadata":{}},{"cell_type":"code","source":"class CallbackConfig:\n    def __init__(self, enabled, **kwargs):\n        self.enabled = enabled\n        self.kwargs = kwargs\n\nclass TrainingConfig:\n    EPOCHS = 30\n    STEPS = 500\n    \n    LEARNING_RATE = 2e-4\n    FINAL_RATE = 1e-5 # enable LR_SCHEDULE to use\n    \n    AUGMENTATIONS = [\n        'brightness',\n        'saturation',\n        'contrast',\n        'color',\n        'translation',\n        'cutout',\n    ]\n    \n    # callbacks\n    CHECKPOINTS = CallbackConfig(\n        enabled=False,\n        # prepend appropriate number of zeros to epoch number in filepath\n        filepath='./checkpoints/{epoch:0%dd}.ckpt' % int(np.ceil(np.log10(EPOCHS+1))),\n    )\n    EVOLUTION_VISUAL = CallbackConfig(\n        enabled=True,\n        classes=['photo', 'monet'],\n        frequency=1,\n    )\n    ALTERNATE = CallbackConfig(\n        enabled=False,\n    )\n    LR_SCHEDULE = CallbackConfig(\n        enabled=True,\n    )","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:28.301825Z","iopub.execute_input":"2022-11-03T17:58:28.302222Z","iopub.status.idle":"2022-11-03T17:58:28.320922Z","shell.execute_reply.started":"2022-11-03T17:58:28.302177Z","shell.execute_reply":"2022-11-03T17:58:28.320017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UnetConfig:\n    UNET_DEPTH = 5\n    BASE_DEPTH = 4\n    CONV_DEPTH = 3\n    \n    # residual or inception\n    BASE_TYPE = 'residual'\n    \n    DROPOUT_RATE = 0.5\n    \n    ACTIVATION = 'leaky_relu'","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:28.325937Z","iopub.execute_input":"2022-11-03T17:58:28.326206Z","iopub.status.idle":"2022-11-03T17:58:28.333188Z","shell.execute_reply.started":"2022-11-03T17:58:28.32618Z","shell.execute_reply":"2022-11-03T17:58:28.332241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatchConfig:\n    DROPOUT_RATE = 0.5","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:28.334931Z","iopub.execute_input":"2022-11-03T17:58:28.335352Z","iopub.status.idle":"2022-11-03T17:58:28.342985Z","shell.execute_reply.started":"2022-11-03T17:58:28.335316Z","shell.execute_reply":"2022-11-03T17:58:28.341962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GANConfig:\n    # weights for generator loss (adversarial, id, cycle)\n    LOSS_WEIGHTS = tf.constant([1e0, 1e0, 1e0])","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:28.344554Z","iopub.execute_input":"2022-11-03T17:58:28.344906Z","iopub.status.idle":"2022-11-03T17:58:31.245933Z","shell.execute_reply.started":"2022-11-03T17:58:28.344872Z","shell.execute_reply":"2022-11-03T17:58:31.244848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VisConfig:\n    # ideal matplotlib figure width for jupyter environment\n    CELL_WIDTH = 16.0","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.247351Z","iopub.execute_input":"2022-11-03T17:58:31.249043Z","iopub.status.idle":"2022-11-03T17:58:31.257314Z","shell.execute_reply.started":"2022-11-03T17:58:31.248995Z","shell.execute_reply":"2022-11-03T17:58:31.256221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataConfig:\n    DB_NAME = 'gan-getting-started'\n    LOCAL_PATH = f'../input/{DB_NAME}'\n    \n    MONET_DIRNAME = 'monet_tfrec'\n    PHOTO_DIRNAME = 'photo_tfrec'\n    DATA_DIRNAMES = [MONET_DIRNAME, PHOTO_DIRNAME]\n    \n    BATCH_SIZE = 8 # 128 # recommended for TPU v3-8 TODO: explore OOM errors\n    BUFFER_SIZE = 300 # size of monet dataset should prevent patterns\n    \n    IMAGE_SHAPE = [256, 256, 3]\n    IMAGE_MAX_RGB = 255\n    \n    SMALLEST_EPOCH = 0\n    LARGEST_EPOCH = 1","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.259985Z","iopub.execute_input":"2022-11-03T17:58:31.260311Z","iopub.status.idle":"2022-11-03T17:58:31.268971Z","shell.execute_reply.started":"2022-11-03T17:58:31.260285Z","shell.execute_reply":"2022-11-03T17:58:31.267955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distributed Computing","metadata":{}},{"cell_type":"code","source":"def distribute_build_strategy():\n    \"\"\"Creates a tf.distribute.Strategy object.\n\n    Assesses the calling compute environment, and creates a Strategy object appropriate\n    given the available accelerators. Preference will be granted to TPU accelerators,\n    followed by GPU accelerators, before falling back on CPU computation.\n\n    Returns:\n        A tf.distribute.Strategy object for the calling compute environment.\n        This will be of one of the following types\n         - TPUStrategy (TPU Accelerator)\n         - MirroredStrategy (GPU Accelerator)\n         - _DefaultDistributionStrategy (No Accelerator)\n    \"\"\"\n    # prefer TPU if available\n    try:\n        # resolver will throw ValueError over the lack of a tpu address if tpu not found\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        # connect to tpu system\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        return tf.distribute.TPUStrategy(tpu)\n    except (ValueError):\n        # resolver will throw ValueError over the lack of a tpu address if tpu not found\n        pass\n\n    # connect to GPU if TPU unavailable\n    if tf.config.list_physical_devices('GPU'):\n        return tf.distribute.MirroredStrategy()\n\n    # fall back on CPU\n    # TODO: evaluate MirroredStrategy for cpu\n    return tf.distribute.get_strategy()","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.270439Z","iopub.execute_input":"2022-11-03T17:58:31.27088Z","iopub.status.idle":"2022-11-03T17:58:31.285694Z","shell.execute_reply.started":"2022-11-03T17:58:31.27084Z","shell.execute_reply":"2022-11-03T17:58:31.284663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distribute_is_tpu(strategy):\n    \"\"\"Evaluates whether the strategy uses a TPU cluster.\n\n    Useful for storage interaction, as the TPU cluster will be in a cloud \n    environment and will not default to local memory.\n\n    Args:\n        strategy (tf.distribute.Strategy): the strategy to be evaluated\n\n    Returns:\n        bool: True if the supplied strategy operates on TPU accelerator(s)\"\"\"\n    return isinstance(strategy, tf.distribute.TPUStrategy)","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.291299Z","iopub.execute_input":"2022-11-03T17:58:31.291675Z","iopub.status.idle":"2022-11-03T17:58:31.297062Z","shell.execute_reply.started":"2022-11-03T17:58:31.291635Z","shell.execute_reply":"2022-11-03T17:58:31.296048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distribute_loss(loss_fn, strategy=None):\n    \"\"\"Wraps a loss function with a strategy-aware reduction.\n\n    Args:\n        loss_fn (Callable[[tf.Tensor, tf.Tensor], tf.Tensor]): The loss \n            function which takes (y_true, y_pred) as arguments and returns\n            the loss. Should be compatible with tensorflow's tf.function api\n            and does not implement any reduction over the batch. For example,\n            a tf.keras.losses.Loss object must be built with the reduction\n            argument set to tf.keras.losses.Reduction.NONE.\n        strategy (tf.distribute.Strategy): The distribution strategy for the \n            calling compute environment. Only needs to be suppplied when\n            function is not called within strategy.scope\n\n    Returns:\n        Callable[[tf.Tensor, tf.Tensor], tf.Tensor]: the loss function wrapped\n            with a strategy-aware reduction. Just like the input loss function,\n            will take (y_true, y_pred) as arguments and return the loss as a\n            Tensor scalar.\n    \"\"\"\n    # capture current strategy\n    if strategy is None:\n        strategy = tf.distribute.get_strategy()\n        \n    # calculate reduction parameters\n    global_batch_size = strategy.num_replicas_in_sync * DataConfig.BATCH_SIZE\n        \n    # build reduction wrapper\n    @tf.function \n    def reduced_loss(y_true, y_pred):\n        # flatten instances\n        flat_shape = (-1, tf.math.reduce_prod(y_true.shape[1:]))\n        y_true = tf.reshape(y_true, flat_shape)\n        y_pred = tf.reshape(y_pred, flat_shape)\n\n        # calculate reduced loss\n        loss_by_instance = loss_fn(y_true, y_pred)\n        return tf.reduce_sum(loss_by_instance) / global_batch_size\n\n    return reduced_loss","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.298888Z","iopub.execute_input":"2022-11-03T17:58:31.299588Z","iopub.status.idle":"2022-11-03T17:58:31.311232Z","shell.execute_reply.started":"2022-11-03T17:58:31.299546Z","shell.execute_reply":"2022-11-03T17:58:31.310272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizations","metadata":{}},{"cell_type":"code","source":"def vis_to_image_grid(images):\n    \"\"\"Transforms tensor of stacked image volumes into a single volume of all images \n    concatenated together.\n\n    This function is not meant to be called directly, as it does not support any variety\n    of tensor shapes. It is called by vis_image_grid, which handles more data validation.\n\n    Args:\n        images (tf.Tensor): tensor of stacked image volumes with rank 5 and indexed by\n            (row, column, height, width, channel).\n\n    Returns:\n        tf.Tensor: tensor of shape (rows*height, cols*width, channel) with all images \n            concatenated into a single volume representing a grid of the images\n    \"\"\"\n    # validate input\n    if len(images.shape) != 5:\n        raise ValueError(f'Invalid tensor rank. Expected rank 5, got {len(images.shape)}')\n\n    # transform tensor into shape (row*height, col*width, channel)\n    images = tf.transpose(images, [0, 2, 1, 3, 4])\n    nshape = (images.shape[0]*images.shape[1], images.shape[2]*images.shape[3], -1)\n    images = tf.reshape(images, nshape)\n\n    # return concatenated images\n    return images","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.312709Z","iopub.execute_input":"2022-11-03T17:58:31.313063Z","iopub.status.idle":"2022-11-03T17:58:31.325088Z","shell.execute_reply.started":"2022-11-03T17:58:31.313027Z","shell.execute_reply":"2022-11-03T17:58:31.324434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vis_image_grid(images):\n    \"\"\"Transforms Tensor of one or more image volumes into a single volume of all images \n    concatenated together.\n\n    This function takes an input Tensor of one of the following shapes:\n        (height, width) - 1 image\n        (height, width, channel) - 1 image\n        (col, height, width, channel) - col images \n        (row, col, height, width, channel) - row*col images\n    The input Tensor is then transformed into a single image volume with all input images\n    concatenated together as a grid. The output image tensor of rank 3 is indexed by\n        (height, width, channel)\n\n    Args:\n        images (tf.Tensor): tensor of one or more stacked image volumes. This tensor must \n            be indexed by one of the following shapes:\n             - (height, width): 1 image\n             - (height, width, channel): 1 image\n             - (col, height, width, channel): row of col images \n             - (row, col, height, width, channel): grid of row*col images\n\n    Returns:\n        tf.Tensor: tensor of shape (rows*height, cols*width, channel) with all images \n            concatenated into a single volume representing a grid of the images\n    \"\"\"\n    # ensure 3d image tensor\n    irank = images.shape\n    if irank == 5:\n        # tensor shape: ()\n        # concatenate images\n        images = vis_to_image_grid(images)\n    elif irank == 4:\n        # assume horizontal and add empty rows axis before concatenating\n        images = images[None]\n        images = vis_to_image_grid(images)\n    elif irank == 2:\n        # add channels axis\n        images = images[..., None]\n    elif irank != 3:\n        raise ValueError(f'Images tensor has improper rank {irank}')\n\n    return images","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.32609Z","iopub.execute_input":"2022-11-03T17:58:31.327549Z","iopub.status.idle":"2022-11-03T17:58:31.34102Z","shell.execute_reply.started":"2022-11-03T17:58:31.327513Z","shell.execute_reply":"2022-11-03T17:58:31.340107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vis_to_pil(img):\n    \"\"\"Transforms input image tensor into a PIL Image object.\n\n    Args:\n        img (Union[tf.Tensor, np.ndarray]): A single image volume of float \n            values in the range [0, 1].\n\n    Returns:\n        PIL.Image: The same image as a PIL.Image object\n    \"\"\"\n    img = tf.cast(img * tf.constant([255.]), tf.uint8)\n    img = img.numpy()\n    img = PIL.Image.fromarray(img)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.342543Z","iopub.execute_input":"2022-11-03T17:58:31.342994Z","iopub.status.idle":"2022-11-03T17:58:31.35498Z","shell.execute_reply.started":"2022-11-03T17:58:31.342958Z","shell.execute_reply":"2022-11-03T17:58:31.353851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vis_image_gallery(\n    images, \n    img_titles=None, \n    row_titles=None,\n    col_titles=None,\n    channels=True,\n):\n    images = np.array(images)\n    nrows, ncols = images.shape[:2]\n    images = images.reshape(-1, *images.shape[2:])\n\n    image_size = VisConfig.CELL_WIDTH / ncols\n    _, axes = plt.subplots(figsize=(image_size*ncols, image_size*nrows), \n                           nrows=nrows, ncols=ncols)\n    axes = np.array(axes).ravel()\n\n    # initialize title arrays to Nones\n    titles  = np.full_like(axes, None)\n    xtitles = np.full_like(axes, None)\n    ytitles = np.full_like(axes, None)\n\n    # evaluate titles\n    if col_titles is not None:\n        titles[:len(col_titles)] = col_titles\n        if img_titles is not None:\n            xtitles[:] = img_titles\n    elif img_titles is not None:\n        titles[:] = img_titles\n    if row_titles is not None:\n        ytitles.reshape(nrows, ncols)[:, 0] = row_titles\n\n    # draw images\n    for img, ax, t, xt, yt in zip(images, axes, titles, xtitles, ytitles):\n        vis_draw_image(img, ax)\n        ax.set_title(t)\n        ax.set_xlabel(xt)\n        ax.set_ylabel(yt)","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.356312Z","iopub.execute_input":"2022-11-03T17:58:31.356826Z","iopub.status.idle":"2022-11-03T17:58:31.370095Z","shell.execute_reply.started":"2022-11-03T17:58:31.356789Z","shell.execute_reply":"2022-11-03T17:58:31.369014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vis_draw_image(img, ax=None):\n    if ax is None:\n        ax = plt.gca()\n\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    for s in ax.spines:\n        ax.spines[s].set_visible(False)\n\n    ax.imshow(img)\n    return ax","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.371483Z","iopub.execute_input":"2022-11-03T17:58:31.372553Z","iopub.status.idle":"2022-11-03T17:58:31.380483Z","shell.execute_reply.started":"2022-11-03T17:58:31.372506Z","shell.execute_reply":"2022-11-03T17:58:31.379448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vis_generator_evolution(\n    images, \n    filepath=None,\n    classes=None,\n    epochs=None,\n    add_alpha=False,\n    separate_classes=True,\n    vertical=False,\n    display=True,\n):\n    \"\"\"Visualizes the training evolution of an image generator.\n\n    TODO: Further description necessary\n\n    Args:\n        images (tf.Tensor): rank 6 tensor of image volumes indexed by\n            (class, image, epoch, height, width, channel)\n        filepath (Optional[str]): filepath to save visual to. \n        classes (Optional[Iterable[str]]): list of classes corresponding\n            to the first index of the images tensor for annotation\n        epochs (Union[None, Iterable[int], Iterable[str]]): list of epochs \n            corresponding to the third index of the images tensor for annotaion\n        separate_classes (bool): If True, a gap will be left between the classes\n        vertical (bool): If True, timeline will extend downward along the visual \n            while classes and images are spread along horizontal axis.\n\n    Returns:\n        tf.Tensor: a single image volume containing the generator evolution\n            visual.\n    \"\"\"\n    # transform each class into an image grid \n    images = [vis_to_image_grid(img_cls) for img_cls in images]\n    # add transparency channel if requested\n    if add_alpha:\n        images = [tf.concat([image, tf.fill((*image.shape[:-1], 1), 1.)], axis=-1) for image in images]\n    # connect all images with gaps between classes\n    final_image = images[0]\n    for img in images[1:]:\n        final_image = tf.concat([\n            final_image, \n            tf.fill((DataConfig.IMAGE_SHAPE[0], *img.shape[1:]), 1.), \n            img\n        ], axis=0)\n\n    if filepath:\n        vis_save_image(final_image, filepath)\n    if display:\n        vis_display_image(final_image)\n    return final_image","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.381896Z","iopub.execute_input":"2022-11-03T17:58:31.382506Z","iopub.status.idle":"2022-11-03T17:58:31.39302Z","shell.execute_reply.started":"2022-11-03T17:58:31.38247Z","shell.execute_reply":"2022-11-03T17:58:31.39212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vis_display_image(img):\n    IPython.display.display(vis_to_pil(img))\n\ndef vis_save_image(img, filepath):\n    vis_to_pil(img).save(filepath)","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.395821Z","iopub.execute_input":"2022-11-03T17:58:31.396107Z","iopub.status.idle":"2022-11-03T17:58:31.408576Z","shell.execute_reply.started":"2022-11-03T17:58:31.396081Z","shell.execute_reply":"2022-11-03T17:58:31.40754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"def data_load(\n    data_path=DataConfig.LOCAL_PATH,\n    data_dirs=DataConfig.DATA_DIRNAMES, \n    batch_size=DataConfig.BATCH_SIZE, \n    buffer_size=DataConfig.BUFFER_SIZE,\n    prefetch=tf.data.AUTOTUNE, \n    repeat=True,\n    seed=None\n):\n    \"\"\"Load competition dataset.\n    \n    Args:\n        data_path (str): The path to the database. If the strategy is using a TPU\n            cluster for distribution, the database should be loaded from the gcs\n            path. If the strategy is using local computation units (GPU or CPU),\n            the database should be loaded from the local path.\n        data_dirs (Iterable[str]): The list of directories within the database to\n            load. Should all be directories containing tfrec files.\n        batch_size (int): The number of instances that should be sampled at a \n            time. Specify batch_size < 1 to disable batching in the dataset.\n        buffer_size (int): The number of instances in a dataset to buffer when\n            shuffling. See tf.data.Dataset.shuffle for more information.\n        prefetch (int): The number of instances to cache ahead of being requested.\n            See tf.data.Dataset.prefetch for more information.\n        repeat (bool): Whether or not each dataset should be repeated arbitrarily\n            many times (e.g., for training purposes).\n        seed (int): The seed supplied to tf.data.Dataset.shuffle. \n        \n    Returns:\n        tf.data.Dataset: A zipped dataset which includes tfrec data read from the \n            supplied directories. Each iteration will yield a tuple of tf.Tensor\n            objects. The first tuple item will be the batch yielded from the \n            first supplied directory, the second item yielded from the second\n            directory, and so on.\n    \"\"\"\n    # append directories to database path\n    data_dirs = (os.path.join(data_path, data_dir) for data_dir in data_dirs)\n\n    # initialize empty list of loaded datasets\n    datasets = []\n    for data_dir in data_dirs:\n        # create and configure dataset object\n        ds = tf.data.TFRecordDataset(tf.io.gfile.glob(f'{data_dir}/*.tfrec'))\n        ds = ds.map(data_tfrec_to_img)\n        ds = ds.prefetch(prefetch).shuffle(buffer_size, seed=seed)\n        if repeat:\n            ds = ds.repeat()\n        if batch_size > 0:\n            ds = ds.batch(batch_size)\n        # append to running list\n        datasets.append(ds)\n\n    # zip all loaded datasets\n    dataset = tf.data.Dataset.zip(tuple(datasets))\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.410183Z","iopub.execute_input":"2022-11-03T17:58:31.410791Z","iopub.status.idle":"2022-11-03T17:58:31.421135Z","shell.execute_reply.started":"2022-11-03T17:58:31.410754Z","shell.execute_reply":"2022-11-03T17:58:31.420454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_load_sample(\n    data_path=DataConfig.LOCAL_PATH,\n    data_dirs=DataConfig.DATA_DIRNAMES, \n    sample_size=DataConfig.BATCH_SIZE, \n    buffer_size=DataConfig.BUFFER_SIZE,\n    seed=None\n):\n    \"\"\"Load a sample from the competition dataset.\n    \n    Args:\n        data_path (str): The path to the database. If the strategy is using a TPU\n            cluster for distribution, the database should be loaded from the gcs\n            path. If the strategy is using local computation units (GPU or CPU),\n            the database should be loaded from the local path.\n        data_dirs (Iterable[str]): The list of directories within the database to\n            load. Should all be directories containing tfrec files.\n        sample_size (int): The number of instances to sample from each directory.\n        buffer_size (int): The number of instances in a dataset to buffer when\n            shuffling. See tf.data.Dataset.shuffle for more information.\n        seed (int): The seed supplied to tf.data.Dataset.shuffle. \n        \n    Returns:\n        Tuple[tf.Tensor]: The samples from each dataset. The first tuple item \n            will be the batch yielded from the first supplied directory, the \n            second item yielded from the second directory, and so on.\n    \"\"\"\n    # append directories to database path\n    data_dirs = (os.path.join(data_path, data_dir) for data_dir in data_dirs)\n\n    # initialize empty list of loaded subsets\n    samples = []\n    for data_dir in data_dirs:\n        # create and configure dataset object\n        ds = tf.data.TFRecordDataset(tf.io.gfile.glob(f'{data_dir}/*.tfrec'))\n        ds = ds.map(data_tfrec_to_img)\n        ds = ds.shuffle(buffer_size, seed=seed)\n        ds = ds.batch(sample_size)\n        # sample and append to running list\n        s = next(iter(ds))\n        samples.append(s)\n    \n    return tuple(samples)","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.422527Z","iopub.execute_input":"2022-11-03T17:58:31.423487Z","iopub.status.idle":"2022-11-03T17:58:31.437899Z","shell.execute_reply.started":"2022-11-03T17:58:31.423447Z","shell.execute_reply":"2022-11-03T17:58:31.436903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_tfrec_to_img(tfrec):\n    \"\"\"Translate a tf record containing a jpeg into the image tensor.\n    \n    Args:\n        tfrec (TFRecord): The tensorflow record object to be parsed.\n        \n    Returns:\n        tf.Tensor[tf.float32]: The image contained in the supplied TFRecord as a\n            tensor of values in the range [0, 1] and with rank 3 shape indexed by\n            (Height, Width, Channels).\n    \"\"\"\n    encoded_image = tf.io.parse_single_example(tfrec, {\n        'image': tf.io.FixedLenFeature([], tf.string)\n    })['image']\n    decoded_image = tf.io.decode_jpeg(encoded_image)\n    return tf.cast(decoded_image, tf.float32) / DataConfig.IMAGE_MAX_RGB","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.440734Z","iopub.execute_input":"2022-11-03T17:58:31.441589Z","iopub.status.idle":"2022-11-03T17:58:31.451754Z","shell.execute_reply.started":"2022-11-03T17:58:31.44155Z","shell.execute_reply":"2022-11-03T17:58:31.450737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_get_path(strategy=None):\n    \"\"\"Retrieves the database path\n    \n    Args:\n        strategy (tf.distribute.Strategy): The distribution strategy for the \n            calling compute environment. Only needs to be suppplied when\n            function is not called within strategy.scope\n    \n    Returns:\n        str: The path to the competition database.\n    \"\"\"\n    if strategy is None:\n        strategy = tf.distribute.get_strategy()\n    if distribute_is_tpu(strategy):\n        return KaggleDatasets().get_gcs_path(DataConfig.DB_NAME)\n    return DataConfig.LOCAL_PATH","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.453043Z","iopub.execute_input":"2022-11-03T17:58:31.454186Z","iopub.status.idle":"2022-11-03T17:58:31.466966Z","shell.execute_reply.started":"2022-11-03T17:58:31.454127Z","shell.execute_reply":"2022-11-03T17:58:31.465869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Augmentations","metadata":{}},{"cell_type":"code","source":"class Augmentor(tf.keras.layers.Layer):\n    def __init__(\n        self, \n        augmentations=TrainingConfig.AUGMENTATIONS,\n        max_brightness_adjustment=0.5,\n        max_saturation_adjustment=0.5,\n        max_contrast_adjustment=0.5,\n        max_color_adjustment=0.1,\n        max_translation_adjustment=0.125,\n        max_cutout_size=0.5,\n        clip_values=True,\n    ):\n        super().__init__()\n        \n        self.max_brightness_adjustment = max_brightness_adjustment\n        self.max_saturation_adjustment = max_saturation_adjustment\n        self.max_contrast_adjustment = max_contrast_adjustment\n        self.max_color_adjustment = max_color_adjustment\n        self.max_translation_adjustment = max_translation_adjustment\n        self.max_cutout_size = max_cutout_size\n        \n        self.clip_values = clip_values\n        \n        if type(augmentations) is str:\n            augmentations = map(str.strip, augmentations.split(','))\n        self.augmentations = list()\n        for augmentation in augmentations:\n            if hasattr(self, augmentation):\n                self.augmentations.append(getattr(self, augmentation))\n    \n    def call(self, *batches):\n        images = tf.concat(batches, 0)\n        \n        # randomize function order in tf.function-compatible manner\n        for i in tf.random.shuffle(tf.range(len(self.augmentations))):\n            for j in range(len(self.augmentations)):\n                if i == j:\n                    images = self.augmentations[j](images)\n        \n        # clipping destroys backprop, but prevents fitting to impossible data\n        if self.clip_values:\n            images = tf.clip_by_value(images, 0., 1.)\n            \n        return tf.split(images, len(batches))\n    \n    def brightness(self, images):\n        # adjust mean pixel brightness\n        num_images = tf.shape(images)[0]\n        adjustment = tf.random.uniform([num_images, 1, 1, 1], -1., 1.) \n        adjustment *= self.max_brightness_adjustment\n        images = images + adjustment\n        return images\n    \n    def color(self, images):\n        # adjust mean of each color\n        num_images = tf.shape(images)[0]\n        adjustment = tf.random.uniform([num_images, 1, 1, 3], -1., 1.)\n        adjustment *= self.max_color_adjustment\n        images = images + adjustment\n        return images\n\n    def contrast(self, images):\n        # adjust variance of pixel brightness\n        num_images = tf.shape(images)[0]\n        adjustment = tf.random.uniform([num_images, 1, 1, 1], -1., 1.)\n        adjustment = 1 + self.max_contrast_adjustment * adjustment\n        brightness = tf.math.reduce_mean(images, axis=[-3, -2, -1], keepdims=True)\n        images = (images - brightness) * adjustment + brightness\n        return images\n\n    def saturation(self, images):\n        # adjust variance of each color\n        num_images = tf.shape(images)[0]\n        adjustment = tf.random.uniform([num_images, 1, 1, 1], -1., 1.)\n        adjustment = 1 + self.max_saturation_adjustment * adjustment\n        avg_colors = tf.math.reduce_mean(images, axis=-1, keepdims=True)\n        images = (images - avg_colors) * adjustment + avg_colors\n        return images\n    \n    # TODO: find a way to sample more than one cutout size per batch\n    # would be easy with for loop... detrimental to GPU / TPU computation?\n    # assumedly, these functions all use loops under the hood anyways\n    def cutout(self, images):\n        num_images = tf.shape(images)[0]\n        image_size = tf.shape(images)[1:3]\n        \n        # sample random cutout locations\n        centers = tf.random.uniform([num_images, 2]) * tf.cast(image_size, tf.float32)\n        centers = tf.cast(centers, tf.int32)\n        \n        # sample random cutout size\n        size = tf.random.uniform([2]) * tf.cast(image_size, tf.float32)\n        size *= self.max_cutout_size\n        \n        # convert to grid indices\n        indices = tf.ragged.range(tf.cast(-size / 2, tf.int32), tf.cast(size / 2 + 0.5, tf.int32))\n        y_indices, x_indices = indices[0], indices[1]\n        y_grid, x_grid = tf.meshgrid(y_indices, x_indices, indexing='ij')\n        grid = tf.stack([y_grid, x_grid], axis=-1)\n        \n        # tile over image index\n        indices = tf.tile(grid[None], [num_images, 1, 1, 1])\n        \n        # offset each image set by centers\n        indices = indices + centers[:, None, None]\n        indices %= image_size\n        \n        # prepend image index \n        image_index = tf.reshape(tf.range(num_images), (-1, 1, 1, 1))\n        bcast_shape = tf.concat([tf.shape(indices)[:-1], [1]], axis=0)\n        image_index = tf.broadcast_to(image_index, bcast_shape)\n        indices = tf.concat([image_index, indices], axis=-1)\n        \n        # reshape to be index list\n        indices = tf.reshape(indices, (-1, tf.shape(indices)[-1]))\n        \n        # mask\n        zeros = tf.zeros((tf.shape(indices)[0], tf.shape(images)[-1]))\n        images = tf.tensor_scatter_nd_update(images, indices, zeros)\n        \n        return images\n\n    def translation(self, images):\n        original_shape = images.shape\n        num_images = tf.shape(images)[0]\n        image_size = tf.shape(images)[1:3]\n        \n        adjustment = tf.random.uniform([num_images, 2], -1., 1.)\n        adjustment *= self.max_translation_adjustment * tf.cast(image_size, tf.float32)\n        adjustment = tf.cast(adjustment + 0.5, tf.int32)\n        adjustment_y, adjustment_x = tf.split(adjustment, 2, axis=1)\n        \n        rows, cols = tf.range(image_size[0])[None], tf.range(image_size[1])[None]\n        rows, cols = tf.tile(rows, [num_images, 1]), tf.tile(cols, [num_images, 1])\n        rows, cols = rows - adjustment_y, cols - adjustment_x\n        \n        # shift indices up by one to separate valid from clipped values\n        rows = tf.clip_by_value(rows+1, 0, image_size[0]+1)[..., None]\n        cols = tf.clip_by_value(cols+1, 0, image_size[1]+1)[..., None]\n        \n        # add padding which will be selected by clipped values\n        aug_images = tf.pad(images, [[0, 0], [1, 1], [1, 1], [0, 0]])\n        aug_images = tf.gather_nd(aug_images, rows, batch_dims=1)\n        aug_images = tf.transpose(aug_images, [0, 2, 1, 3])\n        aug_images = tf.gather_nd(aug_images, cols, batch_dims=1)\n        aug_images = tf.transpose(aug_images, [0, 2, 1, 3])\n        images = tf.ensure_shape(aug_images, images.shape)\n        return images","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.468627Z","iopub.execute_input":"2022-11-03T17:58:31.469776Z","iopub.status.idle":"2022-11-03T17:58:31.499306Z","shell.execute_reply.started":"2022-11-03T17:58:31.469735Z","shell.execute_reply":"2022-11-03T17:58:31.498297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = data_load_sample(data_dirs=[DataConfig.PHOTO_DIRNAME,], sample_size=5)[0]\naug_images = Augmentor()(test_images)[0]\n\nimages = tf.stack([test_images, aug_images])\nimages = tf.transpose(images, [1, 0, 2, 3, 4])\n\nvis_image_gallery(images)\nplt.savefig('example-augmentations.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:31.500939Z","iopub.execute_input":"2022-11-03T17:58:31.501291Z","iopub.status.idle":"2022-11-03T17:58:35.803959Z","shell.execute_reply.started":"2022-11-03T17:58:31.501254Z","shell.execute_reply":"2022-11-03T17:58:35.802625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generator","metadata":{}},{"cell_type":"code","source":"def Unet(\n    input_shape=DataConfig.IMAGE_SHAPE,\n    unet_depth=UnetConfig.UNET_DEPTH, \n    base_depth=UnetConfig.BASE_DEPTH, \n    conv_depth=UnetConfig.CONV_DEPTH, \n    dropout_rate=UnetConfig.DROPOUT_RATE, \n    activation=UnetConfig.ACTIVATION, \n    base=UnetConfig.BASE_TYPE,\n):\n    # Input Layer\n    inputs = tf.keras.Input(input_shape)\n\n    # initialize empty list to track unet skip connections\n    skips = list()\n\n    # Down Stack\n    dn_stack = inputs\n    for level in range(unet_depth):\n        # skip\n        skips.append(dn_stack)\n        # downsample and increase filters\n        filters_in = dn_stack.shape[-1]\n        filters_out = unet_filters_at_level(level+1)\n        dn_stack = unet_downsample(filters_in, filters_out)(dn_stack)\n\n    # Base Stack\n    base_stack = dn_stack\n    base_shape = dn_stack.shape[1:]\n    if base == 'residual':\n        base_stack = unet_residual_base(base_shape, base_depth)(base_stack)\n    # TODO: inception base option\n    elif base == 'inception':\n        base_stack = unet_inception_base(base_shape, base_depth)(base_stack)\n    # TODO: xception base option\n    elif base == 'xception':\n        base_stack = unet_xception_base(base_shape, base_depth)(base_stack)\n\n    # Up Stack\n    up_stack = base_stack\n    for level, skip in reversed(list(enumerate(skips))):\n        # upsample and decrease filters\n        filters_in = up_stack.shape[-1]\n        filters_out = skip.shape[-1]\n        up_stack = unet_upsample(filters_in, filters_out)(up_stack)\n        # concatenate (or add) skip connection along channel axis\n        up_stack = tf.keras.layers.Add()([up_stack, skip])\n\n    # Output\n    outputs = up_stack\n    # convolve reconstructed pixels with originals\n    outputs = unet_upsample(\n        filters_in=3, \n        filters_out=3, \n        dropout_rate=0, \n        sample_rate=1, \n        activation='tanh'\n    )(outputs)\n    # rescale pixel values from [-1, 1] of tanh output to [0, 1]\n    outputs = tf.keras.layers.Rescaling(scale=0.5, offset=0.5)(outputs)\n\n    # Assemble Model\n    unet = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return unet\n\n# TODO: put in primary config for easy experimentation\ndef unet_filters_at_level(level):\n    return min(32 * (2**level), 512)\n\ndef unet_downsample(\n    filters_in,\n    filters_out,\n    depth=3,\n    kernel_size=3,\n    sample_rate=2,\n    dropout_rate=UnetConfig.DROPOUT_RATE,\n    normalize=True,\n    activation='leaky_relu',\n    kernel_initializer='glorot_uniform',\n):\n    block = tf.keras.Sequential()\n    \n    # dropout\n    if dropout_rate:\n        block.add(tf.keras.layers.SpatialDropout2D(dropout_rate))\n\n    # conv layers\n    for _ in range(depth-1):\n        block.add(tf.keras.layers.Conv2D(\n            filters_in,\n            kernel_size,\n            padding='same',\n            kernel_initializer=kernel_initializer\n        ))\n\n    # downsample layer\n    block.add(tf.keras.layers.Conv2D(\n        filters_out,\n        kernel_size,\n        strides=sample_rate,\n        padding='same',\n        kernel_initializer=kernel_initializer,\n    ))\n\n    # normalize\n    if normalize:\n        block.add(tfa.layers.InstanceNormalization())\n\n    # activate\n    if activation:\n        block.add(tf.keras.layers.Activation(activation))\n\n    return block\n\ndef unet_upsample(\n    filters_in,\n    filters_out,\n    depth=3,\n    kernel_size=3,\n    sample_rate=2,\n    dropout_rate=UnetConfig.DROPOUT_RATE,\n    normalize=True,\n    activation='leaky_relu',\n    kernel_initializer='glorot_uniform',\n):\n    block = tf.keras.Sequential()\n    \n    # dropout\n    if dropout_rate:\n        block.add(tf.keras.layers.SpatialDropout2D(dropout_rate))\n\n    # conv layers\n    for _ in range(depth-1):\n        block.add(tf.keras.layers.Conv2D(\n            filters_in,\n            kernel_size,\n            padding='same',\n            kernel_initializer=kernel_initializer,\n        ))\n\n    # upsample layer\n    block.add(tf.keras.layers.Conv2DTranspose(\n        filters_out,\n        kernel_size,\n        strides=sample_rate,\n        padding='same',\n        kernel_initializer=kernel_initializer,\n    ))\n\n    # normalize\n    if normalize:\n        block.add(tfa.layers.InstanceNormalization())\n\n    # activate\n    if activation:\n        block.add(tf.keras.layers.Activation(activation))\n\n    return block\n\ndef unet_residual_base(\n    base_shape, \n    stack_depth, \n    kernel_size=3,\n    dropout_rate=UnetConfig.DROPOUT_RATE,\n    activation=UnetConfig.ACTIVATION, \n    compression_factor=0.5,\n    preactivation=True, \n    bottleneck=True, \n    normalize=True,\n):\n    stack = stack_input = tf.keras.Input(base_shape)\n\n    normalized = (lambda stack: tfa.layers.InstanceNormalization()(stack))\n    activated = (lambda stack: tf.keras.layers.Activation(activation)(stack))\n    actnorm = (lambda stack: activated(normalized(stack)))\n\n    base_filters = base_shape[-1]\n    neck_filters = int(base_filters * compression_factor)        \n    for _ in range(stack_depth):\n        block = block_input = stack\n\n        if dropout_rate:\n            block = tf.keras.layers.SpatialDropout2D(dropout_rate)(block)\n\n        if preactivation:\n            block = activated(block)\n\n        if bottleneck:\n            # compression conv\n            block = tf.keras.layers.Conv2D(neck_filters, 1)(block)\n            block = actnorm(block)\n            # bottleneck conv\n            block = tf.keras.layers.Conv2D(neck_filters, kernel_size, padding='same')(block)\n            block = actnorm(block)\n            # expansion conv\n            block = tf.keras.layers.Conv2D(base_filters, 1)(block)\n            block = normalized(block)\n        else:\n            block = tf.keras.layers.Conv2D(base_filters, kernel_size, padding='same')(block)\n            block = actnorm(block)\n            block = tf.keras.layers.Conv2D(base_filters, kernel_size, padding='same')(block)\n            block = normalized(block)\n\n        stack = tf.keras.layers.Add()([block, block_input])\n\n        if not preactivation:\n            stack = activated(stack)\n    \n    return tf.keras.Model(inputs=stack_input, outputs=stack)","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:35.805524Z","iopub.execute_input":"2022-11-03T17:58:35.806009Z","iopub.status.idle":"2022-11-03T17:58:35.853253Z","shell.execute_reply.started":"2022-11-03T17:58:35.805957Z","shell.execute_reply":"2022-11-03T17:58:35.852103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discriminator ","metadata":{}},{"cell_type":"code","source":"class PatchDiscriminator:\n    def build(\n        dropout_rate=PatchConfig.DROPOUT_RATE,\n    ):\n        \"\"\"\n        :param depth: int. The number of convolutional layers to stack.\n        :param kernel_widths: int, Iterable[int]. The size(s) of the filters.\n        If int, the model will contain depth convolutional layers, each with\n        identical kernel widths as specified. If Iterable[int], their will be \n        len(kernel_widths) convolutional layers with associated kernel widths.\n        :param patch_size: int. The receptive field of each of the neurons in\n        the final output layer.\n        :param init_filters: int. The number of filters in the first convolutional\n        layer. After that, filters will be halved in each layer.\n        :param min_filters: int. Lower bound on number of filters in a convolutional\n        layer. All layers will output at least min_filters channels except for the \n        final layer (which will output one channel as the patch prediction).\n        \"\"\"\n#         # cast kernel_widths to tensor for consistency\n#         if type(kernel_widths) is int:\n#             kernel_widths = tf.fill((depth), kernel_widths)\n#         else:\n#             kernel_widths = tf.constant(kernel_widths)\n#             depth = kernel_widths.shape[0]\n            \n#         # calculate stride for given patch_size, kernel_widths, and depth\n#         strides = tf.constant(kernel_widths)\n        \n#         # reduce number of filters at each layer by factor of 2\n#         channel_depths = init_filters // (2**tf.range(depth))\n#         # apply lower bound\n#         channel_depths = tf.where(channel_depths < min_filters, min_filters, channel_depths)\n\n#         channel_depths=[ 64, 128, 256, 528,   1]\n#         kernel_widths= [  5,   5,   3,   3,   3]\n#         strides=       [  2,   2,   2,   2,   2]\n        # final receptive field of 69x69\n    \n        channel_depths = [ 64, 256, 528,   1]\n        kernel_widths  = [  5,   5,   3,   3]\n        strides        = [  3,   3,   2,   2]\n        # final receptive field of 71x71\n        \n        # build convolutional model\n        layers = [tf.keras.Input(DataConfig.IMAGE_SHAPE)]\n        for filters, width, stride in zip(channel_depths, kernel_widths, strides):\n            layers.extend([\n                tf.keras.layers.SpatialDropout2D(dropout_rate),\n                tf.keras.layers.Conv2D(filters, width, stride),\n                tfa.layers.InstanceNormalization(),\n                tf.keras.layers.Activation('leaky_relu')\n            ])\n        layers[-1] = tf.keras.layers.Activation('sigmoid')\n        \n        model = tf.keras.Sequential(layers)\n        return model","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:35.854805Z","iopub.execute_input":"2022-11-03T17:58:35.855543Z","iopub.status.idle":"2022-11-03T17:58:35.871514Z","shell.execute_reply.started":"2022-11-03T17:58:35.855508Z","shell.execute_reply":"2022-11-03T17:58:35.870644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizers","metadata":{}},{"cell_type":"code","source":"class AdamsFamily:\n    def __init__(self,\n        family_size=4,\n        learning_rate=0.001,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-07,\n        amsgrad=False,\n        name='Adam',\n        **kwargs,\n    ):\n        learning_rate = tf.Variable(learning_rate)\n        self.adams = [tf.keras.optimizers.Adam(\n            learning_rate,\n            beta_1,\n            beta_2,\n            epsilon,\n            amsgrad,\n            name,\n            **kwargs,\n        ) for _ in range(family_size)]\n        self.learning_rate = learning_rate\n        \n    @property\n    def lr(self):\n        return self.learning_rate\n    \n    @lr.setter\n    def lr(self, learning_rate):\n        self.learning_rate.assign(learning_rate)\n        \n    def __iter__(self):\n        self._index = -1\n        return self\n    \n    def __next__(self):\n        self._index += 1\n        return self.adams[self._index]","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:35.873108Z","iopub.execute_input":"2022-11-03T17:58:35.873831Z","iopub.status.idle":"2022-11-03T17:58:35.886542Z","shell.execute_reply.started":"2022-11-03T17:58:35.873795Z","shell.execute_reply":"2022-11-03T17:58:35.885741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CycleGAN","metadata":{}},{"cell_type":"code","source":"class CycleGAN(tf.keras.Model):\n    def __init__(self, m_gen, p_gen, m_dis, p_dis, aug):\n        super().__init__()\n        self.m_gen = m_gen\n        self.p_gen = p_gen\n        self.m_dis = m_dis\n        self.p_dis = p_dis\n        self.aug = aug\n        \n    def compile(self, optimizer, id_loss, cycle_loss, dis_loss, gen_loss_weights):\n        super().compile()\n        self.optimizer = optimizer\n        self.id_loss = id_loss\n        self.cycle_loss = cycle_loss\n        self.dis_loss = dis_loss\n        self.gen_loss_weights = gen_loss_weights\n    \n    def train_step(self, data):\n        # read tupled batch data = (monet_batch, photo_batch)\n        m_real, p_real = data\n        \n        # Progress batch data through CycleGAN process\n        with tf.GradientTape(persistent=True) as g_tape:\n            # identity outputs\n            m_id = self.m_gen(m_real, training=True)\n            p_id = self.p_gen(p_real, training=True)\n            \n            # identity loss\n            m_id_loss = self.id_loss(m_real, m_id)\n            p_id_loss = self.id_loss(p_real, p_id)\n            \n            # transfer outputs\n            m_fake = self.m_gen(p_real, training=True)\n            p_fake = self.p_gen(m_real, training=True)\n            \n            # cycle outputs\n            m_cycle = self.m_gen(p_fake, training=True)\n            p_cycle = self.p_gen(m_fake, training=True)\n            \n            # cycle loss\n            m_cycle_loss = self.cycle_loss(m_real, m_cycle)\n            p_cycle_loss = self.cycle_loss(p_real, p_cycle)\n            cycle_loss = m_cycle_loss + p_cycle_loss\n            \n            # differentiable augmentations\n            m_real, m_fake = self.aug(m_real, m_fake)\n            p_real, p_fake = self.aug(p_real, p_fake)\n            \n            # discriminator outputs\n            m_dis_real = self.m_dis(m_real, training=True)\n            m_dis_fake = self.m_dis(m_fake, training=True)\n            p_dis_real = self.p_dis(p_real, training=True)\n            p_dis_fake = self.p_dis(p_fake, training=True)\n            \n            # discriminator loss            \n            m_dis = tf.concat([m_dis_real, m_dis_fake], 0)\n            p_dis = tf.concat([p_dis_real, p_dis_fake], 0)\n            \n            labels_real =  tf.ones_like(m_dis_real)\n            labels_fake = tf.zeros_like(m_dis_fake)\n            labels = tf.concat([labels_real, labels_fake], 0)\n            \n            m_dis_loss = self.dis_loss(labels, m_dis)\n            p_dis_loss = self.dis_loss(labels, p_dis)\n            \n            # generator loss\n            m_gen_loss = self.dis_loss(labels_real, m_dis_fake)\n            p_gen_loss = self.dis_loss(labels_real, p_dis_fake)\n            \n            m_gen_loss = tf.tensordot(\n                tf.stack([m_gen_loss, m_id_loss, cycle_loss]),\n                self.gen_loss_weights, 1\n            )\n            p_gen_loss = tf.tensordot(\n                tf.stack([p_gen_loss, p_id_loss, cycle_loss]),\n                self.gen_loss_weights, 1\n            )\n            \n        # collect model losses and variables\n        models = [self.m_gen, self.p_gen, self.m_dis, self.p_dis]\n        losses = [m_gen_loss, p_gen_loss, m_dis_loss, p_dis_loss]\n        variables = [model.trainable_variables for model in models]\n        \n        # apply backpropagation\n        for model_loss, model_vars, adam in zip(losses, variables, self.optimizer):\n            grads = g_tape.gradient(model_loss, model_vars)\n            adam.apply_gradients(zip(grads, model_vars))\n        \n        # return losses and metrics\n        return {\n            'monet_id_loss': m_id_loss,\n            'photo_id_loss': p_id_loss,\n            'monet_cycle_loss': m_cycle_loss,\n            'photo_cycle_loss': p_cycle_loss,\n            'monet_discriminator_loss': m_dis_loss,\n            'photo_discriminator_loss': p_dis_loss\n        }\n    \n    def call(self, x, output_class='monet'):\n        if output_class == 'monet':\n            return self.m_gen(x)\n        if output_class == 'photo':\n            return self.p_gen(x)","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:35.896365Z","iopub.execute_input":"2022-11-03T17:58:35.897004Z","iopub.status.idle":"2022-11-03T17:58:35.91885Z","shell.execute_reply.started":"2022-11-03T17:58:35.896968Z","shell.execute_reply":"2022-11-03T17:58:35.917809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Callbacks","metadata":{}},{"cell_type":"code","source":"class VisualizeCycleGanEvolution(tf.keras.callbacks.Callback):\n    DEFAULT_FILEPATH = './cycle-gan-evolution.png'\n    def __init__(self, test_images, classes=None, frequency=1, filepath=DEFAULT_FILEPATH, \n                 separate_classes=True, show_initial=True):\n        \"\"\"\n        :param test_images: tensor containing the batch of images to test on. If multiple \n        classes are being visualized, test_images should be an iterable containing a batch \n        for each class. Index of batches should match the index of the class in classes \n        for which each batch is to be transformed into.\n        :param classes: None, str, or Iterable[str]. The name(s) of the classes explored by\n        the CycleGAN model. Will each be used as an argument to the __call__ method\n        of the CycleGAN. If None, length of classes is assumed to be 1, and the model will\n        be called with no other arguments.\n        :param frequency: int or Iterable[int]. If single int, test will be run at \n        the end of every epoch such that 'epoch % frequency == 0' evaluates to True. If \n        Iterable, test will be run whenever 'epoch in frequency' evaluates to True. Epoch\n        in this consideration will begin at one - not zero.\n        :param filepath: str. The location at which to save the resulting image.\n        :param separate_classes: bool. If true, each class will be saved as a separate \n        image with the class prepended to the file name.\n        :param show_initial: bool. If true, will include initial predictions of the gan \n        model (before any training occurs).\n        \"\"\"\n        super().__init__()\n        \n        # ensure classes is Iterable[str]\n        if classes is None or type(classes) is str:\n            classes = [classes,]\n            \n        # images tensor should be of shape (epoch, class, image, height, width[, channels])\n        if len(classes) == 1:\n            self.images = test_images[None, None]\n        else:\n            self.images = tf.stack(test_images)[None]\n                    \n        # process separate_classes and filepath\n        if separate_classes and len(classes) > 1:\n            name_index = max(0, filepath.rfind('/')+1)\n            self.filepaths = [filepath[:name_index] + class_name + '-' + filepath[name_index:] \n                              for class_name in classes]\n        else:\n            self.filepaths = [filepath,]\n            \n        # assign remaining args to attributes\n        self.classes = classes\n        self.frequency = frequency\n        \n    def on_train_begin(self, logs=None):\n        # collect initial transformations\n        self.images = self._collect_images(self.images)\n    \n    def on_epoch_end(self, epoch, logs=None):\n        # check if frequency dictates this epoch to be detailed\n        epoch += 1\n        if (\n            type(self.frequency) is int and epoch % self.frequency == 0 or\n            hasattr(self.frequency, '__iter__') and epoch in self.frequency\n        ):\n            self.images = self._collect_images(self.images)\n    \n#     @tf.function # TODO: either finish tensorflowizing this, or reformat to need no arguments or returns\n    def _collect_images(self, images):\n        # initialize new tensor with shape (0, height, width[, channels])\n        new_images = tf.zeros([0, *images.shape[3:]], dtype=images.dtype)\n        \n        # iterate over classes and images\n        for c, cla in enumerate(self.classes):\n            # extract original images\n            oimgs = images[0, c]\n            # transform image batch (with class name as argument if available)\n            nimgs = self.model(oimgs, cla) if cla else self.model(oimgs)\n            # concatenate along image axis\n            new_images = tf.concat((new_images, nimgs), axis=0)\n            \n        # add epoch and class axes to tensor\n        new_images = tf.reshape(new_images, (len(self.classes), -1, *new_images.shape[1:]))[None]\n        # concatenate existing epoch data with new\n        return tf.concat((images, new_images), axis=0)\n    \n    def on_train_end(self, logs=None):\n        rank = len(self.images.shape)\n        # ensure channels axis exists\n        if rank == 5:\n            self.images = self.images[..., None]\n            \n        # reshape images from (epoch, class, image, height, width, channels)\n        #                  to (class, image, epoch, height, width, channels)\n        images = tf.transpose(self.images, [1, 2, 0, 3, 4, 5])\n        \n        if len(self.filepaths) > 1:\n            [vis_generator_evolution(img[None], fp) for img, fp in zip(images, self.filepaths)]\n        else:\n            vis_generator_evolution(images, self.filepaths[0])","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:35.920876Z","iopub.execute_input":"2022-11-03T17:58:35.921283Z","iopub.status.idle":"2022-11-03T17:58:35.939285Z","shell.execute_reply.started":"2022-11-03T17:58:35.92119Z","shell.execute_reply":"2022-11-03T17:58:35.938254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AlternateTraining(tf.keras.callbacks.Callback):\n    def on_train_batch_begin(self, batch, logs=None):\n        gen_batch = (batch % 2 == 0)\n        self.model.m_gen.trainable = self.model.p_gen.trainable = gen_batch\n        self.model.m_dis.trainable = self.model.p_dis.trainable = not gen_batch","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:35.942925Z","iopub.execute_input":"2022-11-03T17:58:35.943243Z","iopub.status.idle":"2022-11-03T17:58:35.954674Z","shell.execute_reply.started":"2022-11-03T17:58:35.943215Z","shell.execute_reply":"2022-11-03T17:58:35.95376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementation","metadata":{}},{"cell_type":"code","source":"strategy = distribute_build_strategy()","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:35.95618Z","iopub.execute_input":"2022-11-03T17:58:35.956702Z","iopub.status.idle":"2022-11-03T17:58:35.971467Z","shell.execute_reply.started":"2022-11-03T17:58:35.956665Z","shell.execute_reply":"2022-11-03T17:58:35.970738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensure checkpoints directory exists\nif TrainingConfig.CHECKPOINTS.enabled:\n    checkpoint_fp = TrainingConfig.CHECKPOINTS.kwargs['filepath']\n    checkpoint_dir = os.path.join(os.path.split(checkpoint_fp)[:-1])\n    if not os.path.exists(checkpoint_dir):\n        os.mkdir(checkpoint_dir)\n\ncallbacks = []\nwith strategy.scope(): # for callbacks in need of distribute knowledge (*cough cough* checkpoints *cough*)\n    if TrainingConfig.CHECKPOINTS.enabled:\n        optkwargs = dict(experimental_io_device='/job:localhost')\n        if TrainingConfig.CHECKPOINTS.kwargs.get('save_weights_only', False):\n            options = tf.train.CheckpointOptions(optkwargs)\n        else:\n            options = tf.saved_model.SaveOptions(optkwargs)\n        callbacks.append(tf.keras.callbacks.ModelCheckpoint(\n            **TrainingConfig.CHECKPOINTS.kwargs,\n            options=options,\n        ))\n    if TrainingConfig.EVOLUTION_VISUAL.enabled:\n        callbacks.append(VisualizeCycleGanEvolution(\n            **TrainingConfig.EVOLUTION_VISUAL.kwargs,\n            test_images=data_load_sample(\n                data_get_path(),\n                sample_size=5\n            ),\n        ))\n    if TrainingConfig.ALTERNATE.enabled:\n        callbacks.append(AlternateTraining())\n    if TrainingConfig.LR_SCHEDULE.enabled:\n        def schedule(epoch, lr):\n            \"\"\"Decreases learning rate exponentially according to an initial and final rate\"\"\"\n            LR0 = TrainingConfig.LEARNING_RATE\n            LRF = TrainingConfig.FINAL_RATE\n            EPOCHS = TrainingConfig.EPOCHS\n            attenuation = tf.math.pow(LR0 / LRF, epoch / EPOCHS)\n            lr = LR0 / attenuation\n            return lr\n        callbacks.append(tf.keras.callbacks.LearningRateScheduler(\n            **TrainingConfig.LR_SCHEDULE.kwargs,\n            schedule=schedule\n        ))","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:35.97519Z","iopub.execute_input":"2022-11-03T17:58:35.97662Z","iopub.status.idle":"2022-11-03T17:58:36.94088Z","shell.execute_reply.started":"2022-11-03T17:58:35.976584Z","shell.execute_reply":"2022-11-03T17:58:36.939707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    # instantiate models\n    gan = CycleGAN(\n        m_gen = Unet(),\n        p_gen = Unet(),\n        m_dis = PatchDiscriminator.build(),\n        p_dis = PatchDiscriminator.build(),\n        aug   = Augmentor()\n    )\n    \n    # reduction within distribute strategy restricted only to NONE or SUM\n    NONE=tf.keras.losses.Reduction.NONE\n    \n    # instantiate optimizer and losses\n    gan.compile(\n        optimizer = AdamsFamily(learning_rate=TrainingConfig.LEARNING_RATE),\n        id_loss = distribute_loss(tf.keras.losses.MeanAbsoluteError(reduction=NONE)),\n        cycle_loss = distribute_loss(tf.keras.losses.MeanAbsoluteError(reduction=NONE)),\n        dis_loss = distribute_loss(tf.keras.losses.BinaryCrossentropy(reduction=NONE)),\n        gen_loss_weights = GANConfig.LOSS_WEIGHTS,\n    )","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:36.94495Z","iopub.execute_input":"2022-11-03T17:58:36.945255Z","iopub.status.idle":"2022-11-03T17:58:41.653534Z","shell.execute_reply.started":"2022-11-03T17:58:36.945226Z","shell.execute_reply":"2022-11-03T17:58:41.652561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# document model for journal\n# gan.m_gen.summary()\n# gan.m_dis.summary()\n\ntf.keras.utils.plot_model(gan.m_gen, 'generator_model.png', show_shapes=True, expand_nested=True)\ntf.keras.utils.plot_model(gan.m_dis, 'discriminator_model.png', show_shapes=True, expand_nested=True);","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:41.655016Z","iopub.execute_input":"2022-11-03T17:58:41.65542Z","iopub.status.idle":"2022-11-03T17:58:45.811599Z","shell.execute_reply.started":"2022-11-03T17:58:41.655363Z","shell.execute_reply":"2022-11-03T17:58:45.809976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model\nhistory = gan.fit(\n    x=data_load(data_get_path(strategy)),\n    epochs=TrainingConfig.EPOCHS,\n    steps_per_epoch=TrainingConfig.STEPS,#tf.math.ceil(6686 / data.compute_global_batch_size()),\n    callbacks=callbacks\n)","metadata":{"execution":{"iopub.status.busy":"2022-11-03T17:58:45.813998Z","iopub.execute_input":"2022-11-03T17:58:45.814457Z","iopub.status.idle":"2022-11-03T18:00:46.571615Z","shell.execute_reply.started":"2022-11-03T17:58:45.814391Z","shell.execute_reply":"2022-11-03T18:00:46.569799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Images","metadata":{}},{"cell_type":"code","source":"photos = data_load(\n    data_get_path(strategy), \n    data_dirs=[DataConfig.PHOTO_DIRNAME,], \n    buffer_size=1,\n    repeat=False,\n)\n\nos.mkdir('../fresh-monets')\nfilepath='../fresh-monets/{:04d}.png'\n\nfor batch_num, photo_batch in enumerate(photos):\n    monet_batch = gan(photo_batch)\n    image_offset = batch_num * DataConfig.BATCH_SIZE\n    for image_num, monet in enumerate(monet_batch):\n        vis_save_image(monet[0], filepath.format(image_offset + image_num))\n    \nshutil.make_archive('./images', 'zip', '../fresh-monets');","metadata":{"execution":{"iopub.status.busy":"2022-11-03T18:00:46.581582Z","iopub.execute_input":"2022-11-03T18:00:46.582432Z","iopub.status.idle":"2022-11-03T18:02:21.626603Z","shell.execute_reply.started":"2022-11-03T18:00:46.582355Z","shell.execute_reply":"2022-11-03T18:02:21.625528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"photos = data_load_sample(\n    data_get_path(strategy), \n    data_dirs=[DataConfig.PHOTO_DIRNAME,], \n    sample_size=5,\n    seed=0,\n)[0]\n\nmonets = gan(photos)\n\nimages = np.array([photos, monets]).swapaxes(0, 1)\n\nvis_image_gallery(images, col_titles=['Photo', 'Monet'])\nplt.savefig('final-test.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-03T18:02:21.628482Z","iopub.execute_input":"2022-11-03T18:02:21.628902Z","iopub.status.idle":"2022-11-03T18:02:24.732871Z","shell.execute_reply.started":"2022-11-03T18:02:21.628862Z","shell.execute_reply":"2022-11-03T18:02:24.731483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir('./models')\ngan.m_gen.save('./models/monet-generator')\ngan.p_gen.save('./models/photo-generator')\ngan.m_dis.save('./models/monet-discriminator')\ngan.p_dis.save('./models/photo-discriminator')","metadata":{"execution":{"iopub.status.busy":"2022-11-03T18:04:21.248075Z","iopub.execute_input":"2022-11-03T18:04:21.249039Z","iopub.status.idle":"2022-11-03T18:05:08.821496Z","shell.execute_reply.started":"2022-11-03T18:04:21.249Z","shell.execute_reply":"2022-11-03T18:05:08.820355Z"},"trusted":true},"execution_count":null,"outputs":[]}]}