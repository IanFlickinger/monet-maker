Results-06:
Results-06-01: Shows the effect of adding an alternated training approach
Results-06-02: Experiments on normalization in between the input and its skip connection

Results-07: Bad config. Ran for 1 epoch of 1 step.

Results-08: Added a 1x1 convolution between output and input in generator architecture

Results-09: PatchGAN architecture changes

Results-10: Error occurred during runtime

Results-11: Weighted generator loss

Results-12: Dropout; No normalization in up stack

Results-13: Error occurred during runtime

Results-14: Bad config. Trained for 15 epochs of one step.

Results-15: 

Results-16: Spatial dropout, increased filters, deeper u-net, shallower base stack

Results-17: Replaced layer normalization with instance normalization

Results-18: Initial experiments with spatial dropout and instance normalization.

Results-19: Error occurred during runtime

Results-20: Dense spatial dropout distributed through generator feature extraction.

Results-21: No dropout

Results-22: Spatial Dropout in Discriminator as well as in the Generator Base and Up Stack

Results-23: Weighted the adversarial loss over the identity and cycle losses

Results-24: Replaced Concat layers with Add in Unet architecture

Results-25: LeakyReLU in generator base, and reorganized upsample/downsample blocks