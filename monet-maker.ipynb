{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nimport numpy as np\n\n# visualization\nimport matplotlib.pyplot as plt\nimport PIL\nimport IPython\n\n# environment configuration\nimport kaggle_datasets\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:27.588321Z","iopub.execute_input":"2022-10-29T03:20:27.588779Z","iopub.status.idle":"2022-10-29T03:20:27.595559Z","shell.execute_reply.started":"2022-10-29T03:20:27.58874Z","shell.execute_reply":"2022-10-29T03:20:27.594045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mute tensorflow logging\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = str(3)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:27.598079Z","iopub.execute_input":"2022-10-29T03:20:27.59985Z","iopub.status.idle":"2022-10-29T03:20:27.608024Z","shell.execute_reply.started":"2022-10-29T03:20:27.599817Z","shell.execute_reply":"2022-10-29T03:20:27.60708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configs","metadata":{}},{"cell_type":"markdown","source":"# Distributed Computing","metadata":{}},{"cell_type":"code","source":"class distribute:\n    \n    def build_strategy():\n        \"\"\"Creates a tf.distribute.Strategy object.\n        \n        Assesses the calling compute environment, and creates a Strategy object appropriate\n        given the available accelerators. Preference will be granted to TPU accelerators,\n        followed by GPU accelerators, before falling back on CPU computation.\n        \n        Returns:\n            A tf.distribute.Strategy object for the calling compute environment.\n            This will be of one of the following types\n             - TPUStrategy (TPU Accelerator)\n             - MirroredStrategy (GPU Accelerator)\n             - _DefaultDistributionStrategy (No Accelerator)\n        \"\"\"\n        # prefer TPU if available\n        try:\n            # resolver will throw ValueError over the lack of a tpu address if tpu not found\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n            # connect to tpu system\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            return tf.distribute.TPUStrategy(tpu)\n        except (ValueError):\n            # resolver will throw ValueError over the lack of a tpu address if tpu not found\n            pass\n        \n        # connect to GPU if TPU unavailable\n        if tf.config.list_physical_devices('GPU'):\n            return tf.distribute.MirroredStrategy()\n        \n        # fall back on CPU\n        # TODO: evaluate MirroredStrategy for cpu\n        return tf.distribute.get_strategy()\n            \n    def is_tpu(strategy):\n        \"\"\"Evaluates whether the strategy uses a TPU cluster.\n        \n        Useful for storage interaction, as the TPU cluster will be in a cloud \n        environment and will not default to local memory.\n        \n        Args:\n            strategy (tf.distribute.Strategy): the strategy to be evaluated\n        \n        Returns:\n            bool: True if the supplied strategy operates on TPU accelerator(s)\"\"\"\n        return isinstance(strategy, tf.distribute.TPUStrategy)\n    \n    def loss(strategy, loss_fn):\n        \"\"\"Wraps a loss function with a strategy-aware reduction.\n        \n        Args:\n            strategy (tf.distribute.Strategy): The distribution strategy for the \n                calling compute environment.\n            loss_fn (Callable[[tf.Tensor, tf.Tensor], tf.Tensor]): The loss \n                function which takes (y_true, y_pred) as arguments and returns\n                the loss.\n        \n        Returns:\n            Callable[[tf.Tensor, tf.Tensor], tf.Tensor]: the loss function wrapped\n                with a strategy-aware reduction. Just as the input loss function,\n                will take (y_true, y_pred) as arguments and return the loss.\n        \"\"\"\n        @tf.function # TODO: tf-ize the function... e.g., check data.compute_global_batch_size effect\n        def reduced_loss(y_true, y_pred):\n            # flatten instances\n            flat_shape = (-1, tf.math.reduce_prod(y_true.shape[1:]))\n            y_true = tf.reshape(y_true, flat_shape)\n            y_pred = tf.reshape(y_pred, flat_shape)\n            \n            # calculate reduced loss\n            loss_by_instance = loss_fn(y_true, y_pred)\n            global_batch_size = data.compute_global_batch_size() # TODO: add reference to strategy arg\n            return tf.reduce_sum(loss_by_instance) / global_batch_size\n        \n        return reduced_loss","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:27.610414Z","iopub.execute_input":"2022-10-29T03:20:27.612207Z","iopub.status.idle":"2022-10-29T03:20:27.642021Z","shell.execute_reply.started":"2022-10-29T03:20:27.61185Z","shell.execute_reply":"2022-10-29T03:20:27.640515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizations","metadata":{}},{"cell_type":"code","source":"class vis:\n    max_width = 16.0\n        \n    def _to_image_grid(images):\n        \"\"\"Transforms tensor of stacked image volumes into a single volume of all images \n        concatenated together.\n        \n        This function is not meant to be called directly, as it does not support any variety\n        of tensor shapes. It is called by vis.image_grid, which handles more data validation.\n        \n        Args:\n            images (tf.Tensor): tensor of stacked image volumes with rank 5 and indexed by\n                (row, column, height, width, channel).\n                \n        Returns:\n            tf.Tensor: tensor of shape (rows*height, cols*width, channel) with all images \n                concatenated into a single volume representing a grid of the images\n        \"\"\"\n        # validate input\n        if len(images.shape) != 5:\n            raise ValueError(f'Invalid tensor rank. Expected rank 5, got {len(images.shape)}')\n        \n        # transform tensor into shape (row*height, col*width, channel)\n        images = tf.transpose(images, [0, 2, 1, 3, 4])\n        nshape = (images.shape[0]*images.shape[1], images.shape[2]*images.shape[3], -1)\n        images = tf.reshape(images, nshape)\n        \n        # return concatenated images\n        return images\n    \n    def image_grid(images):\n        \"\"\"Transforms Tensor of one or more image volumes into a single volume of all images \n        concatenated together.\n        \n        This function takes an input Tensor of one of the following shapes:\n            (height, width) - 1 image\n            (height, width, channel) - 1 image\n            (col, height, width, channel) - col images \n            (row, col, height, width, channel) - row*col images\n        The input Tensor is then transformed into a single image volume with all input images\n        concatenated together as a grid. The output image tensor of rank 3 is indexed by\n            (height, width, channel)\n            \n        Args:\n            images (tf.Tensor): tensor of one or more stacked image volumes. This tensor must \n                be indexed by one of the following shapes:\n                 - (height, width): 1 image\n                 - (height, width, channel): 1 image\n                 - (col, height, width, channel): row of col images \n                 - (row, col, height, width, channel): grid of row*col images\n        \n        Returns:\n            tf.Tensor: tensor of shape (rows*height, cols*width, channel) with all images \n                concatenated into a single volume representing a grid of the images\n        \"\"\"\n        # ensure 3d image tensor\n        irank = images.shape\n        if irank == 5:\n            # tensor shape: ()\n            # concatenate images\n            images = vis._to_image_grid(images)\n        elif irank == 4:\n            # assume horizontal and add empty rows axis before concatenating\n            images = images[None]\n            images = vis._to_image_grid(images)\n        elif irank == 2:\n            # add channels axis\n            images = images[..., None]\n        elif irank != 3:\n            raise ValueError(f'Images tensor has improper rank {irank}')\n            \n        return images\n    \n    def to_pil(img):\n        \"\"\"Transforms input image tensor into a PIL Image object.\n        \n        Args:\n            img (Union[tf.Tensor, np.ndarray]): A single image volume of float \n                values in the range [0, 1].\n        \n        Returns:\n            PIL.Image: The same image as a PIL.Image object\n        \"\"\"\n        img = tf.cast(img * tf.constant([255.]), tf.uint8)\n        img = img.numpy()\n        img = PIL.Image.fromarray(img)\n        return img\n    \n    def image_gallery(\n        images, \n        img_titles=None, \n        row_titles=None,\n        col_titles=None,\n        channels=True,\n    ):\n        images = np.array(images)\n        nrows, ncols = images.shape[:2]\n        images = images.reshape(-1, *images.shape[2:])\n        \n        image_size = vis.max_width / ncols\n        _, axes = plt.subplots(figsize=(image_size*ncols, image_size*nrows), \n                               nrows=nrows, ncols=ncols)\n        axes = np.array(axes).ravel()\n        \n        # initialize title arrays to Nones\n        titles  = np.full_like(axes, None)\n        xtitles = np.full_like(axes, None)\n        ytitles = np.full_like(axes, None)\n        \n        # evaluate titles\n        if col_titles is not None:\n            titles[:len(col_titles)] = col_titles\n            if img_titles is not None:\n                xtitles[:] = img_titles\n        elif img_titles is not None:\n            titles[:] = img_titles\n        if row_titles is not None:\n            ytitles.reshape(nrows, ncols)[:, 0] = row_titles\n        \n        # draw images\n        for img, ax, t, xt, yt in zip(images, axes, titles, xtitles, ytitles):\n            vis.draw_image(img, ax)\n            ax.set_title(t)\n            ax.set_xlabel(xt)\n            ax.set_ylabel(yt)\n        \n    def draw_image(img, ax=None):\n        if ax is None:\n            ax = plt.gca()\n        \n        ax.set_xticks([])\n        ax.set_yticks([])\n        \n        for s in ax.spines:\n            ax.spines[s].set_visible(False)\n            \n        ax.imshow(img)\n        return ax\n    \n    def generator_evolution(\n        images, \n        filepath=None,\n        classes=None,\n        epochs=None,\n        add_alpha=True,\n        separate_classes=True,\n        vertical=False,\n        display=True,\n    ):\n        \"\"\"Visualizes the training evolution of an image generator.\n        \n        TODO: Further description necessary\n        \n        Args:\n            images (tf.Tensor): rank 6 tensor of image volumes indexed by\n                (class, image, epoch, height, width, channel)\n            filepath (Optional[str]): filepath to save visual to. \n            classes (Optional[Iterable[str]]): list of classes corresponding\n                to the first index of the images tensor for annotation\n            epochs (Union[None, Iterable[int], Iterable[str]]): list of epochs \n                corresponding to the third index of the images tensor for annotaion\n            separate_classes (bool): If True, a gap will be left between the classes\n            vertical (bool): If True, timeline will extend downward along the visual \n                while classes and images are spread along horizontal axis.\n                \n        Returns:\n            tf.Tensor: a single image volume containing the generator evolution\n                visual.\n        \"\"\"\n        # transform each class into an image grid \n        images = [vis._to_image_grid(img_cls) for img_cls in images]\n        # add transparency channel if requested\n        if add_alpha:\n            images = [tf.concat([image, tf.fill((*image.shape[:-1], 1), 1.)], axis=-1) for image in images]\n        # connect all images with gaps between classes\n        final_image = images[0]\n        for img in images[1:]:\n            final_image = tf.concat([\n                final_image, \n                tf.fill((data.image_shape[0], *img.shape[1:]), 1.), \n                img\n            ], axis=0)\n            \n        if filepath:\n            vis.save_image(final_image, filepath)\n        if display:\n            vis.display_image(final_image)\n        return final_image\n        \n    \n    def display_image(img):\n        IPython.display.display(vis.to_pil(img))\n        \n    def save_image(img, filepath):\n        vis.to_pil(img).save(filepath)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:27.649537Z","iopub.execute_input":"2022-10-29T03:20:27.65127Z","iopub.status.idle":"2022-10-29T03:20:27.707587Z","shell.execute_reply.started":"2022-10-29T03:20:27.651217Z","shell.execute_reply":"2022-10-29T03:20:27.706216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"class data:\n    db_name    = 'gan-getting-started'\n    local_path = f'../input/{db_name}'\n    monet_dir  = '/monet_tfrec'\n    photo_dir  = '/photo_tfrec'\n    data_dirs  = [monet_dir, photo_dir]\n    \n    batch_size = 8 # 128 # recommended for TPU v3-8... but what about GPU and CPU???\n    prefetch = batch_size # will not be scaled with num_replicas_in_sync... problem???\n    buffer_size = 300 # size of monet dataset should prevent patterns\n    \n    image_shape = [256, 256, 3]\n    image_max_rgb = 255\n    \n    SMALLEST_EPOCH = 0\n    LARGEST_EPOCH = 1\n    \n    def load(\n        data_path=local_path,\n        data_dirs=data_dirs, \n        batch_size=batch_size, \n        prefetch=prefetch, \n        buffer_size=buffer_size,\n        zip_method=LARGEST_EPOCH,\n        seed=None\n    ):\n        # append directories to database path\n        data_dirs = map(lambda data_dir: data_path + data_dir, data_dirs)\n        \n        # initialize list of loaded datasets\n        datasets = []\n        for data_dir in data_dirs:\n            # read in tf records\n            tfds = tf.data.TFRecordDataset(tf.io.gfile.glob(f'{data_dir}/*.tfrec'))\n            # map to actual images\n            ds = tfds.map(data._tfrec_to_img)\n            # configure dataset options\n            ds = ds.prefetch(prefetch).shuffle(buffer_size, seed=seed)\n            if zip_method == data.LARGEST_EPOCH:\n                ds = ds.repeat()\n            if batch_size > 0:\n                ds = ds.batch(batch_size)\n            # append to list of loaded datasets\n            datasets.append(ds)\n        \n        dataset = tf.data.Dataset.zip(tuple(datasets))\n        \n        return dataset\n    \n    def load_subset(\n        data_path=local_path,\n        data_dirs=data_dirs, \n        subset_size=1,\n        batch_size=batch_size, \n        buffer_size=buffer_size,\n        seed=None\n    ):\n        # append directories to database path\n        data_dirs = map(lambda data_dir: data_path + data_dir, data_dirs)\n        \n        # initialize list of loaded subsets\n        subsets = []\n        for data_dir in data_dirs:\n            # read in tf records\n            tfds = tf.data.TFRecordDataset(tf.io.gfile.glob(f'{data_dir}/*.tfrec'))\n            # map to actual images\n            ds = tfds.map(data._tfrec_to_img)\n            # configure dataset options\n            ds = ds.shuffle(buffer_size, seed=seed)\n            if batch_size > 0:\n                ds = ds.batch(batch_size)\n            # sample subset_size batches\n            ss = tf.data.Dataset.from_tensor_slices([batch for _, batch in zip(range(subset_size), ds)])\n            # append sampled batches to list of loaded subsets\n            subsets.append(ss)\n        \n        # zip loaded subsets into one\n        if len(subsets) > 1:\n            subset = tf.data.Dataset.zip(tuple(subsets))\n        else:\n            subset = subsets[0]\n        \n        return subset\n            \n    def _tfrec_to_img(tfrec):\n        encoded_image = tf.io.parse_single_example(tfrec, {\n            'image': tf.io.FixedLenFeature([], tf.string)\n        })['image']\n        decoded_image = tf.io.decode_jpeg(encoded_image)\n        return tf.cast(decoded_image, tf.float32) / data.image_max_rgb\n    \n    def compute_global_batch_size(strategy=None, local_batch_size=batch_size):\n        if strategy is None:\n            return local_batch_size\n        return strategy.num_replicas_in_sync * local_batch_size\n    \n    def get_data_path(tpu_strategy=False):\n        if tpu_strategy:\n            return KaggleDatasets().get_gcs_path(data.db_name)\n        return data.local_path","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:27.709173Z","iopub.execute_input":"2022-10-29T03:20:27.709991Z","iopub.status.idle":"2022-10-29T03:20:27.743927Z","shell.execute_reply.started":"2022-10-29T03:20:27.709956Z","shell.execute_reply":"2022-10-29T03:20:27.742771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentor","metadata":{}},{"cell_type":"code","source":"class Augmentor(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n        self.augmentations = []\n    \n    def call(self, batch1, batch2):\n        images = tf.concat([batch1, batch2], 0)\n        \n        for aug in self.augmentations:\n            images = aug(images)\n        \n        return tf.split(images, 2)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:27.745337Z","iopub.execute_input":"2022-10-29T03:20:27.746358Z","iopub.status.idle":"2022-10-29T03:20:27.756547Z","shell.execute_reply.started":"2022-10-29T03:20:27.746322Z","shell.execute_reply":"2022-10-29T03:20:27.755387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generator","metadata":{}},{"cell_type":"code","source":"class UnetGenerator:\n    def build(\n        unet_depth=5, \n        base_depth=4, \n        conv_depth=2, \n        dropout_rate=0.5,\n        activation='leaky_relu', \n        base='residual', \n        name=None,\n    ):\n        # Input Layer\n        inputs = tf.keras.Input(data.image_shape, name='input_image')\n        \n        # Down Stack\n        dn_stack = inputs\n        skips = []\n        for level in range(unet_depth):\n            # convolve \n            filters = UnetGenerator.filters_at_level(level)\n            dn_stack = UnetGenerator.conv(dn_stack, filters, conv_depth)\n            # skip\n            skips.append(dn_stack)\n            # downsample\n            dn_stack = UnetGenerator.downsample(dn_stack)\n            \n        # Base Stack\n        base_stack = dn_stack\n        if base == 'residual':\n            base_stack = UnetGenerator.residual_base(base_stack, base_depth, dropout_rate=dropout_rate)\n        elif base == 'inception':\n            base_stack = UnetGenerator.inception_base(base_stack, base_depth, dropout_rate=dropout_rate)\n\n        # Up Stack\n        up_stack = base_stack\n        for level, skip in reversed(list(enumerate(skips))):\n            # upsample\n            up_stack = UnetGenerator.upsample(up_stack)\n            # concatenate skip connection along channel axis\n            up_stack = tf.keras.layers.Concatenate(axis=-1)([up_stack, skip])\n            # convolve skip and up_stack tensors\n            filters = UnetGenerator.filters_at_level(level)\n            up_stack = UnetGenerator.conv(up_stack, filters, conv_depth, dropout_rate=dropout_rate)\n            \n        # Output\n        outputs = up_stack\n        # convolve back to three channels\n        outputs = UnetGenerator.conv(outputs, 3, conv_depth)\n        # concatenate original image\n        outputs = tf.keras.layers.Concatenate(axis=-1)([outputs, inputs])\n        # convolve reconstructed pixels with original for coloration\n        outputs = tf.keras.layers.Conv2D(\n            filters=3, \n            kernel_size=1, \n            padding='same',            \n            activation='tanh'\n        )(outputs)\n        # rescale pixel values from [-1, 1] of tanh output to [0, 1]\n        outputs = tf.keras.layers.Rescaling(scale=0.5, offset=0.5)(outputs)\n        \n        # Assemble Model\n        unet = tf.keras.Model(inputs=inputs, outputs=outputs, name=name)\n        return unet\n    \n    def filters_at_level(level):\n        return min(32 * (2**level), 512)\n    \n    def downsample(\n        input_tensor, \n        size=3, \n        strides=2, \n        activation=None, \n        normalize=False,\n        initializer='glorot_uniform'\n    ):\n        # downsampling layer\n        ds = tf.keras.layers.Conv2D(\n            input_tensor.shape[-1] * 2,\n            size, \n            strides, \n            padding='same', \n            kernel_initializer=initializer\n        )(input_tensor)\n            \n        # normalization\n        if normalize:\n            ds = tfa.layers.InstanceNormalization()(ds)\n        \n        # activation\n        if activation:\n            ds = tf.keras.layers.Activation(activation)(ds)\n            \n        return ds\n    \n    def upsample(\n        input_tensor, \n        size=3, \n        strides=2, \n        activation=None, \n        normalize=False,\n        initializer='glorot_uniform'\n    ):\n        # upsampling layer\n        us = tf.keras.layers.Conv2DTranspose(\n            input_tensor.shape[-1] // 2,\n            size, \n            strides, \n            padding='same', \n            kernel_initializer=initializer\n        )(input_tensor)\n            \n        # normalization\n        if normalize:\n            us = tfa.layers.InstanceNormalization()(us)\n        \n        # activation\n        if activation:\n            us = tf.keras.layers.Activation(activation)(us)\n            \n        return us\n    \n    def conv(\n        input_tensor, \n        filters=-1,\n        depth=2, \n        size=3, \n        dropout_rate=0,\n        activation='leaky_relu',\n        normalize=True\n    ):\n        \"\"\"\n        :param level\n        :param size\n        :param depth\n        :param activation: str or callable used as argument to keras Activation layer.\n        Supply activation=None to remove activation layer.\n        :param normalize\n        \n        Builds several consecutive Conv2D blocks, and adds optional activation \n        and normalization layers.\n        \n        Based on the premise of increasing effective kernel size with fewer parameters\n        as introduced by VGGNets.\n        \"\"\"\n        block = input_tensor\n        \n        # dropout\n        if dropout_rate:\n            block = tf.keras.layers.SpatialDropout2D(dropout_rate)(block)\n        \n        # convolution layers\n        if filters < 0:\n            # maintain input channels when in doubt\n            filters = input_tensor.shape[-1]\n        for _ in range(depth):\n            block = tf.keras.layers.Conv2D(filters, size, padding='same')(block)\n            \n        # normalization\n        if normalize:\n            # use LayerNormalization to mimic instance normalization (available in tensorflow addons)\n            block = tfa.layers.InstanceNormalization()(block)\n        \n        # activation\n        if activation:\n            block = tf.keras.layers.Activation(activation)(block)\n            \n        return block\n    \n    def residual_base(\n        input_tensor, \n        depth, \n        size=3,\n        dropout_rate=0,\n        activation='relu', \n        preactivation=True, \n        bottleneck=True, \n        compression_factor=0.5,\n        normalize=True\n    ):\n        stack = input_tensor\n        \n        # use LayerNormalization to mimic instance normalization (available in tensorflow addons)\n        normalized = (lambda stack: tfa.layers.InstanceNormalization()(stack))\n        activated = (lambda stack: tf.keras.layers.Activation(activation)(stack))\n        actnorm = (lambda stack: activated(normalized(stack)))\n        \n        filters = input_tensor.shape[-1]\n        neck_filters = int(filters * compression_factor)        \n        for _ in range(depth):\n            stack_input = stack\n        \n            if dropout_rate:\n                stack = tf.keras.layers.SpatialDropout2D(dropout_rate)(stack)\n                \n            if preactivation:\n                stack = activated(stack)\n                \n            if bottleneck:\n                # compression conv\n                stack = tf.keras.layers.Conv2D(neck_filters, 1)(stack)\n                stack = actnorm(stack)\n                # bottleneck conv\n                stack = tf.keras.layers.Conv2D(neck_filters, size, padding='same')(stack)\n                stack = actnorm(stack)\n                # expansion conv\n                stack = tf.keras.layers.Conv2D(filters, 1)(stack)\n                stack = normalized(stack)\n            else:\n                stack = tf.keras.layers.Conv2D(filters, size, padding='same')(stack)\n                stack = actnorm(stack)\n                stack = tf.keras.layers.Conv2D(filters, size, padding='same')(stack)\n                stack = normalized(stack)\n            \n            stack = tf.keras.layers.Add()([stack, stack_input])\n            \n            if not preactivation:\n                stack = activated(stack)\n                \n        return stack","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:27.759304Z","iopub.execute_input":"2022-10-29T03:20:27.76023Z","iopub.status.idle":"2022-10-29T03:20:27.80867Z","shell.execute_reply.started":"2022-10-29T03:20:27.760178Z","shell.execute_reply":"2022-10-29T03:20:27.807186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discriminator ","metadata":{}},{"cell_type":"code","source":"class PatchDiscriminator:\n    def build(depth=3, kernel_widths=3, patch_size=70, init_filters=128, min_filters=8, dropout_rate=0.5):\n        \"\"\"\n        :param depth: int. The number of convolutional layers to stack.\n        :param kernel_widths: int, Iterable[int]. The size(s) of the filters.\n        If int, the model will contain depth convolutional layers, each with\n        identical kernel widths as specified. If Iterable[int], their will be \n        len(kernel_widths) convolutional layers with associated kernel widths.\n        :param patch_size: int. The receptive field of each of the neurons in\n        the final output layer.\n        :param init_filters: int. The number of filters in the first convolutional\n        layer. After that, filters will be halved in each layer.\n        :param min_filters: int. Lower bound on number of filters in a convolutional\n        layer. All layers will output at least min_filters channels except for the \n        final layer (which will output one channel as the patch prediction).\n        \"\"\"\n#         # cast kernel_widths to tensor for consistency\n#         if type(kernel_widths) is int:\n#             kernel_widths = tf.fill((depth), kernel_widths)\n#         else:\n#             kernel_widths = tf.constant(kernel_widths)\n#             depth = kernel_widths.shape[0]\n            \n#         # calculate stride for given patch_size, kernel_widths, and depth\n#         strides = tf.constant(kernel_widths)\n        \n#         # reduce number of filters at each layer by factor of 2\n#         channel_depths = init_filters // (2**tf.range(depth))\n#         # apply lower bound\n#         channel_depths = tf.where(channel_depths < min_filters, min_filters, channel_depths)\n\n        channel_depths=[ 64, 128, 256, 528,   1]\n        kernel_widths= [  5,   5,   3,   3,   3]\n        strides=       [  2,   2,   2,   2,   2]\n        # final receptive field of 69x69\n        \n        # build convolutional model\n        layers = [tf.keras.Input(data.image_shape)]\n        for filters, width, stride in zip(channel_depths, kernel_widths, strides):\n            layers.extend([\n                tf.keras.layers.SpatialDropout2D(dropout_rate),\n                tf.keras.layers.Conv2D(filters, width, stride),\n                tfa.layers.InstanceNormalization(),\n                tf.keras.layers.Activation('leaky_relu')\n            ])\n        layers[-1] = tf.keras.layers.Activation('sigmoid')\n        \n        model = tf.keras.Sequential(layers)\n        return model","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:27.815816Z","iopub.execute_input":"2022-10-29T03:20:27.818555Z","iopub.status.idle":"2022-10-29T03:20:27.831975Z","shell.execute_reply.started":"2022-10-29T03:20:27.818517Z","shell.execute_reply":"2022-10-29T03:20:27.830881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CycleGAN","metadata":{}},{"cell_type":"code","source":"class CycleGAN(tf.keras.Model):\n    def __init__(self, m_gen, p_gen, m_dis, p_dis, aug):\n        super().__init__()\n        self.m_gen = m_gen\n        self.p_gen = p_gen\n        self.m_dis = m_dis\n        self.p_dis = p_dis\n        self.aug = aug\n        \n    def compile(self, optimizer, id_loss, cycle_loss, dis_loss):\n        super().compile()\n        self.optimizer = optimizer\n        self.id_loss = id_loss\n        self.cycle_loss = cycle_loss\n        self.dis_loss = dis_loss\n    \n    def train_step(self, data):\n        # read tupled batch data = (monet_batch, photo_batch)\n        m_real, p_real = data\n        \n        # Progress batch data through CycleGAN process\n        with tf.GradientTape(persistent=True) as g_tape:\n            # identity outputs\n            m_id = self.m_gen(m_real, training=True)\n            p_id = self.p_gen(p_real, training=True)\n            \n            # identity loss\n            m_id_loss = self.id_loss(m_real, m_id)\n            p_id_loss = self.id_loss(p_real, p_id)\n            \n            # transfer outputs\n            m_fake = self.m_gen(p_real, training=True)\n            p_fake = self.p_gen(m_real, training=True)\n            \n            # cycle outputs\n            m_cycle = self.m_gen(p_fake, training=True)\n            p_cycle = self.p_gen(m_fake, training=True)\n            \n            # cycle loss\n            m_cycle_loss = self.cycle_loss(m_real, m_cycle)\n            p_cycle_loss = self.cycle_loss(p_real, p_cycle)\n            cycle_loss = m_cycle_loss + p_cycle_loss\n            \n            # differentiable augmentations\n            m_real, m_fake = self.aug(m_real, m_fake)\n            p_real, p_fake = self.aug(p_real, p_fake)\n            \n            # discriminator outputs\n            m_dis_real = self.m_dis(m_real, training=True)\n            m_dis_fake = self.m_dis(m_fake, training=True)\n            p_dis_real = self.p_dis(p_real, training=True)\n            p_dis_fake = self.p_dis(p_fake, training=True)\n            \n            # discriminator loss            \n            m_dis = tf.concat([m_dis_real, m_dis_fake], 0)\n            p_dis = tf.concat([p_dis_real, p_dis_fake], 0)\n            \n            labels_real =  tf.ones_like(m_dis_real)\n            labels_fake = tf.zeros_like(m_dis_fake)\n            labels = tf.concat([labels_real, labels_fake], 0)\n            \n            m_dis_loss = self.dis_loss(labels, m_dis)\n            p_dis_loss = self.dis_loss(labels, p_dis)\n            \n            # generator loss\n            labels = tf.concat([labels_fake, labels_real], 0)\n            m_gen_loss = self.dis_loss(labels, m_dis)\n            p_gen_loss = self.dis_loss(labels, p_dis)\n            \n            m_gen_loss += m_id_loss + cycle_loss \n            p_gen_loss += p_id_loss + cycle_loss \n            \n        # collect model losses and variables\n        models = [self.m_gen, self.p_gen, self.m_dis, self.p_dis]\n        losses = [m_gen_loss, p_gen_loss, m_dis_loss, p_dis_loss]\n        variables = [model.trainable_variables for model in models]\n        \n        # apply backpropagation\n        for model_loss, model_vars in zip(losses, variables):\n            grads = g_tape.gradient(model_loss, model_vars)\n            self.optimizer.apply_gradients(zip(grads, model_vars))\n        \n        # return losses and metrics\n        return {\n            'monet_id_loss': m_id_loss,\n            'photo_id_loss': p_id_loss,\n            'monet_cycle_loss': m_cycle_loss,\n            'photo_cycle_loss': p_cycle_loss,\n            'monet_discriminator_loss': m_dis_loss,\n            'photo_discriminator_loss': p_dis_loss\n        }\n    \n    def call(self, x, output_class='monet'):\n        if output_class == 'monet':\n            return self.m_gen(x)\n        if output_class == 'photo':\n            return self.p_gen(x)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:27.837278Z","iopub.execute_input":"2022-10-29T03:20:27.840173Z","iopub.status.idle":"2022-10-29T03:20:27.872477Z","shell.execute_reply.started":"2022-10-29T03:20:27.840134Z","shell.execute_reply":"2022-10-29T03:20:27.871601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VisualizeCycleGanEvolution(tf.keras.callbacks.Callback):\n    default_filepath='./cycle-gan-evolution.png'\n    def __init__(self, test_images, classes=None, frequency=1, filepath=default_filepath, \n                 separate_classes=True, show_initial=True):\n        \"\"\"\n        :param test_images: tensor containing the batch of images to test on. If multiple \n        classes are being visualized, test_images should be an iterable containing a batch \n        for each class. Index of batches should match the index of the class in classes \n        for which each batch is to be transformed into.\n        :param classes: None, str, or Iterable[str]. The name(s) of the classes explored by\n        the CycleGAN model. Will each be used as an argument to the __call__ method\n        of the CycleGAN. If None, length of classes is assumed to be 1, and the model will\n        be called with no other arguments.\n        :param frequency: int or Iterable[int]. If single int, test will be run at \n        the end of every epoch such that 'epoch % frequency == 0' evaluates to True. If \n        Iterable, test will be run whenever 'epoch in frequency' evaluates to True. Epoch\n        in this consideration will begin at one - not zero.\n        :param filepath: str. The location at which to save the resulting image.\n        :param separate_classes: bool. If true, each class will be saved as a separate \n        image with the class prepended to the file name.\n        :param show_initial: bool. If true, will include initial predictions of the gan \n        model (before any training occurs).\n        \"\"\"\n        super().__init__()\n        \n        # ensure classes is Iterable[str]\n        if classes is None or type(classes) is str:\n            classes = [classes,]\n            \n        # images tensor should be of shape (epoch, class, image, height, width[, channels])\n        if len(classes) == 1:\n            self.images = test_images[None, None]\n        else:\n            self.images = tf.stack(test_images)[None]\n                    \n        # process separate_classes and filepath\n        if separate_classes and len(classes) > 1:\n            name_index = max(0, filepath.rfind('/')+1)\n            self.filepaths = [filepath[:name_index] + class_name + '-' + filepath[name_index:] \n                              for class_name in classes]\n        else:\n            self.filepaths = [filepath,]\n            \n        # assign remaining args to attributes\n        self.classes = classes\n        self.frequency = frequency\n        \n    def on_train_begin(self, logs=None):\n        # collect initial transformations\n        self.images = self._collect_images(self.images)\n    \n    def on_epoch_end(self, epoch, logs=None):\n        # check if frequency dictates this epoch to be detailed\n        epoch += 1\n        if (\n            type(self.frequency) is int and epoch % self.frequency == 0 or\n            hasattr(self.frequency, '__iter__') and epoch in self.frequency\n        ):\n            self.images = self._collect_images(self.images)\n    \n#     @tf.function # TODO: either finish tensorflowizing this, or reformat to need no arguments or returns\n    def _collect_images(self, images):\n        # initialize new tensor with shape (0, height, width[, channels])\n        new_images = tf.zeros([0, *images.shape[3:]], dtype=images.dtype)\n        \n        # iterate over classes and images\n        for c, cla in enumerate(self.classes):\n            # extract original images\n            oimgs = images[0, c]\n            # transform image batch (with class name as argument if available)\n            nimgs = self.model(oimgs, cla) if cla else self.model(oimgs)\n            # concatenate along image axis\n            new_images = tf.concat((new_images, nimgs), axis=0)\n            \n        # add epoch and class axes to tensor\n        new_images = tf.reshape(new_images, (len(self.classes), -1, *new_images.shape[1:]))[None]\n        # concatenate existing epoch data with new\n        return tf.concat((images, new_images), axis=0)\n    \n    def on_train_end(self, logs=None):\n        rank = len(self.images.shape)\n        # ensure channels axis exists\n        if rank == 5:\n            self.images = self.images[..., None]\n            \n        # reshape images from (epoch, class, image, height, width, channels)\n        #                  to (class, image, epoch, height, width, channels)\n        images = tf.transpose(self.images, [1, 2, 0, 3, 4, 5])\n        \n        if len(self.filepaths) > 1:\n            [vis.generator_evolution(img[None], fp) for img, fp in zip(images, self.filepaths)]\n        else:\n            vis.generator_evolution(images, self.filepaths[0])","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:27.877519Z","iopub.execute_input":"2022-10-29T03:20:27.877901Z","iopub.status.idle":"2022-10-29T03:20:27.907194Z","shell.execute_reply.started":"2022-10-29T03:20:27.877865Z","shell.execute_reply":"2022-10-29T03:20:27.906023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AlternateTraining(tf.keras.callbacks.Callback):\n    def on_train_batch_begin(self, batch, logs=None):\n        gen_batch = (batch % 2 == 0)\n        self.model.m_gen.trainable = self.model.p_gen.trainable = gen_batch\n        self.model.m_dis.trainable = self.model.p_dis.trainable = not gen_batch","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:27.909138Z","iopub.execute_input":"2022-10-29T03:20:27.90961Z","iopub.status.idle":"2022-10-29T03:20:27.922492Z","shell.execute_reply.started":"2022-10-29T03:20:27.909498Z","shell.execute_reply":"2022-10-29T03:20:27.921162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementation","metadata":{}},{"cell_type":"code","source":"strategy = distribute.build_strategy()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:27.924505Z","iopub.execute_input":"2022-10-29T03:20:27.924847Z","iopub.status.idle":"2022-10-29T03:20:31.027129Z","shell.execute_reply.started":"2022-10-29T03:20:27.924788Z","shell.execute_reply":"2022-10-29T03:20:31.026168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    # instantiate models\n    gan = CycleGAN(\n        m_gen = UnetGenerator.build(),\n        p_gen = UnetGenerator.build(),\n        m_dis = PatchDiscriminator.build(),\n        p_dis = PatchDiscriminator.build(),\n        aug   = Augmentor()\n    )\n    \n    # reduction within distribute strategy restricted only to NONE or SUM\n    NONE=tf.keras.losses.Reduction.NONE\n    \n    # instantiate optimizer and losses\n    gan.compile(\n        optimizer = tf.keras.optimizers.Adam(2e-4),\n        id_loss = distribute.loss(strategy, tf.keras.losses.MeanAbsoluteError(reduction=NONE)),\n        cycle_loss = distribute.loss(strategy, tf.keras.losses.MeanAbsoluteError(reduction=NONE)),\n        dis_loss = distribute.loss(strategy, tf.keras.losses.BinaryCrossentropy(reduction=NONE))\n    )","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:31.028755Z","iopub.execute_input":"2022-10-29T03:20:31.029477Z","iopub.status.idle":"2022-10-29T03:20:34.123348Z","shell.execute_reply.started":"2022-10-29T03:20:31.029422Z","shell.execute_reply":"2022-10-29T03:20:34.122297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensure checkpoints directory exists\n# if not os.path.exists('./checkpoints'):\n#     os.mkdir('./checkpoints')\n\nwith strategy.scope(): # for callbacks in need of distribute knowledge (*cough cough* checkpoints *cough*)\n    callbacks = [\n#         tf.keras.callbacks.ModelCheckpoint(\n#             filepath='./checkpoints/{epoch:02d}.ckpt',\n#             save_weights_only=True,\n#             options=tf.train.CheckpointOptions(experimental_io_device='/job:localhost')\n#         ),\n        VisualizeCycleGanEvolution(\n            test_images=next(iter(data.load_subset(\n                data.get_data_path(distribute.is_tpu(strategy)),\n                batch_size=5\n            ))),\n            classes=['photo', 'monet'],\n            frequency=3,\n        ),\n#         AlternateTraining(),\n    ]","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:34.12537Z","iopub.execute_input":"2022-10-29T03:20:34.126009Z","iopub.status.idle":"2022-10-29T03:20:36.339559Z","shell.execute_reply.started":"2022-10-29T03:20:34.125955Z","shell.execute_reply":"2022-10-29T03:20:36.338492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# document model for journal\n# gan.m_gen.summary()\n# gan.m_dis.summary()\n\ntf.keras.utils.plot_model(gan.m_gen, 'generator_model.png', show_shapes=True)\ntf.keras.utils.plot_model(gan.m_dis, 'discriminator_model.png', show_shapes=True);","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:36.34091Z","iopub.execute_input":"2022-10-29T03:20:36.34181Z","iopub.status.idle":"2022-10-29T03:20:39.569275Z","shell.execute_reply.started":"2022-10-29T03:20:36.341765Z","shell.execute_reply":"2022-10-29T03:20:39.567868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model\nhistory = gan.fit(\n    x=data.load(data.get_data_path(distribute.is_tpu(strategy))),\n    epochs=24,\n    initial_epoch=0,\n    steps_per_epoch=300,#tf.math.ceil(6686 / data.compute_global_batch_size()),\n    validation_data=None,\n    validation_steps=None,\n    validation_freq=1,\n    callbacks=callbacks\n)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:20:39.571545Z","iopub.execute_input":"2022-10-29T03:20:39.572008Z","iopub.status.idle":"2022-10-29T03:22:32.218428Z","shell.execute_reply.started":"2022-10-29T03:20:39.571956Z","shell.execute_reply":"2022-10-29T03:22:32.216907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Images","metadata":{}},{"cell_type":"code","source":"test_set = data.load_subset(\n    data.get_data_path(distribute.is_tpu(strategy)),\n    data_dirs=[data.photo_dir,],\n    batch_size=5,\n    seed=0\n)\n\nphotos = next(iter(test_set))\nmonets = gan(photos)\n\nimages = np.array([photos, monets]).swapaxes(0, 1)\n\ncol_titles = ['Photo', 'Monet']\nvis.image_gallery(images, col_titles=col_titles)\nplt.savefig('final-test.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T03:22:32.22086Z","iopub.execute_input":"2022-10-29T03:22:32.221852Z","iopub.status.idle":"2022-10-29T03:22:35.873938Z","shell.execute_reply.started":"2022-10-29T03:22:32.221789Z","shell.execute_reply":"2022-10-29T03:22:35.872625Z"},"trusted":true},"execution_count":null,"outputs":[]}]}